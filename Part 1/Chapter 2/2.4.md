# 神经网络的“引擎”：基于梯度的优化

步骤：
- 随机初始化权重
- 循环训练
  - 抽取批量数据
  - 前向传播
  - 计算损失
  - 更新权重

更新权重方案：
-  尝试改变某个标量系数，保持其他权重不变
-  利用网络运算可微，计算损失相对于网络系数的梯度（gradient），向梯度反方向改变系数，从而降低损失
## 2.4.1 什么是导数
## 2.4.2 张量运算的导数：梯度
梯度（gradient）是导数这一概念向多元函数导数的推广
## 2.4.3 随机梯度下降
步骤：
- 抽取训练样本x和对应目标y组成的数据批量
- 在x上运行网络，得到预测值y_pred
- 计算网络在这批数据上的损失，用于平衡y_pred和y之间的距离
- 计算损失相对于网络参数的梯度【一次反向传播（backward pass）】
- 将参数沿着梯度的反方向移动一点，比如 W -= step * gradient

上述方法叫做小批量随机梯度下降（）mini-batch stochastic gradient desent，又称为小批量SGD）  
随机（stochastic）：随机抽取数据  

真SGD：每次迭代只抽取一个样本和目标  

批量SGD：每次迭代在所有数据上运行  

优化方法（optimization method）或优化器（optimizer）：SGD变种  
区别：计算下一次权重更新时考虑上一次权重更新  
例如：带动量的SGD、Adagrad、RMSProp  

带动量的SGD：可以有效避免局部极小点  
实现：  

```python
past_velocity = 0.  # 初始动量
momentum = 0.1      # 动量因子（定值）
while loss > 0.01:  # 优化循环
    w, loss, gradient = get_current_parameters()    # 获取当前参数、损失、梯度
    velocity = past_velocity * momentum - learning_rate * gradient  # 根据前一动量计算当前动量
    w = w + momentum * velocity - learning_rate * gradient  # 计算新权重
    past_velocity = velocity    # 更新动量
    update_parameter(w) #更新参数
```
## 2.4.4 链式求导：反向传播算法
f(W1, W2, W3) = a(W1, b(W2, c(W3)))  

链式法则（chain rule）：（f(g(x))）'=f'(g(x)) * g'(x)  

反向传播（backpropagation）或 反式微分（reverse-rule differentiation）：将链式法则应用于神经网络梯度值计算得到的算法
