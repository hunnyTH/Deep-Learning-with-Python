# 3.1 神经网络剖析
要点：
- 层：多个层组合成网络（模型）
- 输入数据和相应的目标
- 损失函数，即用于学习的反馈信号
- 优化器，决定学习过程如何进行

## 3.1.1 层：深度学习的基础组件
权重：层的状态  

密集连接层（densely connected layer）：使用形状为（samples, features）的2D张量保存简单向量数据。又称全连接层（fully connected layer）或密集层（dense layer）  

循环层（recurrent layer）：使用形状为（samples, timesteps, features）的3D张量保存序列数据  

卷积层：使用4D张量保存图像数据  

层兼容性：每一层只接收特定形状的输入张量，并返回特定形状的输出张量。例如
```python
from keras import layers

layer = layers.Dense(32, input_shape=(784,))
```
该层接受第一个维度大小为784的2D张量（第0轴是批量维度，大小未指定，取任意值）作为输入。返回一个张量，第一个维度为32

## 3.1.2 模型：层构成的网络
深度学习模型是层构成的有向无环图：线性堆叠、将单一输入映射为单一输出  

网络拓扑结构：
- 双分支（two-branch）网络
- 多头（multihead）网络
- Inception模块

网络拓扑结构定义了一个假设空间（hypothesis space）

## 3.1.3 损失函数与优化器：配置学习过程的关键
损失函数（目标函数）：训练过程中将其最小化，衡量当前任务是否已成功完成  

优化器：决定如何基于损失函数对网络进行更新，它执行的是随机梯度下降（SGD）的某个变体

具有多个输出的神经网络可能就具有多个损失函数（每个输出对应一个损失函数），但是梯度下降过程必须基于单个标量损失值，因此对于该网络须将损失函数取平均  

明智地选择目标函数：
- 二分类问题：二元交叉熵（binary cross entropy）
- 多分类问题：分类交叉熵（categorical cross entropy）
- 回归问题：均方误差（mean-squared error）
- 序列学习问题：联结主义时序分类（CTC, connectionist temporal classification）
